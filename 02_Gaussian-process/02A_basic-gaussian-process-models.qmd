---
title: "Basic Gaussian Process Models"
subtitle: |
date: '`r Sys.Date()`'
author: Nam-Anh Tran
format:
  html: 
    theme: darkly
    toc: false
fontsize: 12pt
reference-location: margin
citation-location: margin
number-sections: true
fig-cap-location: bottom
fig-height: 4
fig-width: 5
link-citations: true
linkcolor: deeppink
indent: true
classoption: [onecolumn, portrait]
bibliography: 02A_references.bib
csl: ../vancouver.csl
editor: source
editor_options: 
  chunk_output_type: console
execute: 
  eval: false
---

\renewcommand{\bf}[1]{\boldsymbol{#1}}
\doublespacing

## Basic idea of Gaussian process (GP) models

Let's consider the following function 

$$
y = \sin x, \quad x \in [0,2\pi] .
$$

We now assume that we do not know this function but we observe $\boldsymbol{x}$ and $\boldsymbol y$ shown in @fig-01. 

```{r}
#| eval: true
#| label: fig-01
#| fig-cap: seven observation $(x_i,y_i), i = 1,2,\dots,8$.

if(!library(pacman, logical.return = T)) install.packages("pacman")
pacman::p_load(tidyverse, plgp, laGP, mvtnorm)

n = 10
x_obs = matrix(seq(0,2*pi,length=n), ncol=1)
y_obs = sin(x_obs)
plot(x_obs, y_obs, ylim = c(-5,5), xlab = "observation x", ylab = expression(sin~"(x)" ) )

# gp = newGP(x_obs, y, 2, 1e-6, dK = TRUE)
# mleGP(gp, tmax = 20)
# jmleGP(gp)
```

Our objective is to model this data using the Gaussian process framework. Suppose $Y_i$, corresponding to $x_i$, is normally distributed with mean $\mu$ and SD $\sigma$. We also assume that $Y_i$ and $Y_j$ are correlated with corresponding covariance $\sigma_{ij}$ dependent on the distance of $x_i$ and $x_j$ (the further the weaker). We now describe this setup using the gaussian process notation 

$$
\boldsymbol Y \sim GP(\boldsymbol Y; \boldsymbol\mu, \boldsymbol \Sigma),
$$ {#eq-01}

which has the same meaning with the multivariate normal framework, i.e. $Y_n \sim N_n(\mu, \Sigma_n)$. Without loss of generality, we assume that $\boldsymbol \mu = \boldsymbol 0$, i.e.

$$
\boldsymbol Y \sim GP(\boldsymbol Y; \boldsymbol 0, \boldsymbol \Sigma)
$$ {#eq-02}

We now simulate $\boldsymbol Y$ using the multivariate normal distribution

```{r}
#| eval: true
#| label: fig-02
#| fig-cap: Prior Gaussian.

set.seed(111)
n  = 100
x_pr = matrix(seq(-0.5, 2*pi+0.5, length=100), ncol=1)
d_pr = distance(x_pr)
Sigma_pr = exp(-d_pr)
mu_pr = numeric(length(x_pr))|> as.matrix()

y_pr = mvtnorm::rmvnorm(3, mu_pr, Sigma_pr)
matplot(x_pr, t(y_pr), type = "l", lty = 1, xlab = "prior x", ylab = "density")
```

Inherited from the conditional multivariate normal distribution, we determine the posterior distribution which is the conditional distribution $Y^{prior}|Y^{obs} \equiv Y^{pos}$. 


```{r}
#| label: fig-03
#| eval: true
#| fig-cap: "The predictive surface interpolates the data."

d_obs = distance(x_obs)
Sigma_obs = exp(-d_obs)
kap = distance(x_pr, x_obs)|> {\(i) exp(-i)}()

mu_pos = mu_pr + kap%*%solve(Sigma_obs)%*%(y_obs)
Sigma_pos = Sigma_pr - kap%*%solve(Sigma_obs)%*%t(kap)

y_pos = mvtnorm::rmvnorm(10000, mu_pos, Sigma_pos)|> t()
ci = apply(y_pos,1, HDInterval::hdi)

matplot(x_pr, t(ci), type = "l", col = "black", lty = 1, ylim = c(-5,5), xlab = "observation x", ylab = expression(sin~"(x)" ) )
for(i in 1:100) lines(x_pr, y_pos[,i], col = "gray")
matplot(x_pr, t(ci), col = "black", lty = 1, add = T, type = "l")
#lines(x, m )
lines(x_obs, y_obs, pch = 16, col = "red", type = "o")
```












