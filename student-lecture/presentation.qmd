---
title: "Bayesian model selection for generalized linear mixed models"
subtitle: 'Authors: @xu2023bayesian'
author: "Presenters: Qicheng Zhao, Nam-Anh Tran"
format: 
  revealjs:
    theme: league
    transition: fade
editor: source
fontsize: 19.5pt
execute: 
  echo: false
editor_options: 
  chunk_output_type: console
bibliography: ../references.bib
---


# Agenda 

1. Theory
   - Pseudo likelihood function for generalized linear mixed models
   - Model selection
2. Simulation study 

# Pseudo likelihood function for generalized linear mixed models

## GLMM

Consider the model:

$$
f(y_i|\eta_i) = \exp[y_i\eta_i - B_i(\eta_i) + C_i(y_i)], \quad i = 1,2,\dots,n
$$

where $\mu_i = B'_i(\eta_i)$ and $v_i = B_i''(\eta_i)$, and 

$$
\eta_i = \boldsymbol x_i^{\top}\boldsymbol\beta + \sum_j^Q\boldsymbol z_{ij}^{\top}\boldsymbol\alpha_j
$$

Let $\boldsymbol\alpha$ is a vector of random effects. Then 

$$
\boldsymbol\alpha|\boldsymbol\tau \sim N(\boldsymbol 0, \boldsymbol\tau\boldsymbol\Sigma)
$$

- If $\boldsymbol\alpha$ is a vector of spatial random effects that follows a sum-zero constrained Gaussian intrinsic conditional autoregressive model (ICAR).

## How to define $\boldsymbol\Sigma$ for the sum-zero constrained Gaussian intrinsic conditional autoregressive model?  


- Define adjacency matrix $\boldsymbol W = [w_{ij}]$, where $w_{ij} = 1$ if region $i$ and $j$ are neighbours, and 0 otherwise. 
- Let $\boldsymbol D_w$ is the diagonal matrix. The diagonal element is the row sum of $\boldsymbol W$ 
- $\boldsymbol \Sigma$ is the pseudo-inverse of $\boldsymbol D_w -\boldsymbol W$.


## Pseudo likelihood function for GLMMs

- A key step in Bayesian model selection is to integrate out random effects from the likelihood function, which does not have a analytical form for GLMMs.
- Pseudo-likelihood approach that approximates a GLMM for non-Gaussian data by computing adjusted observations that are modeled using an approximate Gaussian LMM.

Specifically, we calculate this intractable likelihood function

![](eq01.png){width=55% fig-align="center"}


## Pseudo likelihood function for GLMMs

- The pseudo-likelihood approach is an iterative procedure.

- We start with the model: 

$$
\boldsymbol y = \boldsymbol\mu + \boldsymbol\epsilon,
$$ {#eq-01}

and assume $\widehat{\boldsymbol\alpha}, \widehat{\boldsymbol\beta}, \widehat{\boldsymbol\mu}$ and $\widehat{\boldsymbol V}$ are estimates of their corresponding parameters.

- Using Taylor expansion (i.e. $f(x) \approx f(x_0) + f'(x_0)(x-x_0)$) around $\boldsymbol\alpha$ and $\boldsymbol\beta$ in @eq-01, we have

$$
\begin{aligned}
LHS &= 
\widehat{\boldsymbol V}^{-1}(y - \widehat{\boldsymbol\mu}) + \widehat{\boldsymbol V}\widehat{\boldsymbol\mu}'[\boldsymbol X\widehat{\boldsymbol\beta} + \sum_j\boldsymbol Z_j\widehat{\boldsymbol\alpha}_j]\\
&=\widehat{\boldsymbol V}^{-1}(y - \widehat{\boldsymbol\mu}) + \boldsymbol X\widehat{\boldsymbol\beta} + \sum_j\boldsymbol Z_j\widehat{\boldsymbol\alpha}_j \\
&\doteq \boldsymbol y^*
\end{aligned}
$$

$\boldsymbol y^*$ is a vector of adjusted observations.


## Pseudo likelihood function for GLMMs

- Equating $\boldsymbol y^*$ to the RHS, we have 

$$
\boldsymbol y^* \approx \boldsymbol{X\beta} + \sum_j \boldsymbol Z_j\boldsymbol\alpha_j + \widehat{\boldsymbol V}^{-1}\boldsymbol\epsilon,
$$ {#eq-02}

where $\boldsymbol\alpha_j \sim N(\boldsymbol 0, \tau_j\boldsymbol\Sigma_j)$, and $\boldsymbol\epsilon \sim N(\boldsymbol 0, \boldsymbol V)$.

Substitute $\boldsymbol V$ with $\widehat{\boldsymbol V}$ in @eq-02, we have 

$$
\boldsymbol y^* \sim N(\boldsymbol{X\beta}, \sum_j\tau_j\boldsymbol Z_j\boldsymbol\Sigma_j\boldsymbol Z_j^{\top} + \widehat{\boldsymbol V}^{-1})
$$

# Model selection 

## Posterior model probabilities

- Consider the model space $\mathfrak{M} = \{ M_c\}_{c=1}^C$, where $M_c$ has $K_c$ regressors and $\boldsymbol X_c$ is the corresponding matrix of predictors, and $\boldsymbol\beta_c$ is the corresponding vector of coefficients. 

- We integrate the likelihood based on $\boldsymbol y^*$:

$$
p(\boldsymbol y^*|M_c) = \int_{\Theta} p(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi(\boldsymbol\beta_c,\tau_c|M_c)d\boldsymbol\beta_cd\boldsymbol\tau_c
$$ {#eq-03}

- Let $\pi(M_c)$ be the prior probability of $M_c$, we have 

$$
P(M_c|\boldsymbol y^*) =\frac{ p(\boldsymbol y^*|M_c)\pi(M_c)}{\sum_{r=1}^C p(\boldsymbol y^*|M_c)\pi(M_c)}
\propto \color{pink}{{p(\boldsymbol y^*|M_c)\pi(M_c)}}
$$ {#eq-04}

- @eq-04 is our primary objective. 

- We then need to define the priors for $\boldsymbol\beta, \boldsymbol\tau$ and $M_c$. 

## Priors for model parameters

- Two parameters of interest are $\boldsymbol\beta$ and precision $1/\boldsymbol\tau$ where $\boldsymbol\beta \perp \boldsymbol\tau$.

- $\boldsymbol\beta$ follows Uniform distribution on $\mathbb{R}^p$, i.e. $\boldsymbol\beta|M \propto 1$, the flat prior.  
- $\pi(\tau) \propto (1+\frac{\tau}{\alpha_{\tau}})^{-2}$, $\alpha_{\tau}$ is a hyperparameter.
- By simulation, authors show that $\alpha_{\tau} = 2$ works well for GLMMs. Thus, 

$$
\pi_1(\tau|M) = \frac{1}{2(\tau/2+1)^2}, \tau \ge 0.
$$

Another choice is the half-Cauchy for $\sqrt{\tau}$

$$
\pi_2(\tau) \propto \frac{1}{(\tau + 1)\sqrt{\tau}},
$$
$\sqrt{\tau}$ has more mass near zero and more mass for large value of $\tau$. 

## Priors on the model space

::: {.panel-tabset}

### A 

- Let $K$ denote the number of candidate covariates and $Q$ denote the number of candidate random effects types. Also $K_c$ is the number of covariates  in the model $M_c$. 
- The prior probability for model $M_c$ with $K_c$ covariates is 

$$
P(M_c\text{ with } K_c \text{ covariates}) = \frac{1}{(K+1){K\choose K_c}}
$$

- For random effect, there are $2^Q$ possibilities for inclusion and exclusion of random effects. 
- If $P(\text{"inclusion"}) = 0.5$, then $P(M_c\text{ with }Q_c\text{ types of random effects}) = \frac{1}{2^Q}$
- Assuming a priori independence of inclusion of fixed effects and random effects, the prior probability for model $M_c$ is

$$
P(M_c) = \frac{1}{2^Q(K+1){K\choose K_c}}
$$


### B

Let's consider $\{x_1,x_2,x_3\}$, we have 

- Intercept only, i.e. $M1$
- 1 covariate: $x_1, x_2, x_3$, i.e $M2, M3, M4$
- 2 covariates: $x_1x_2, x_2x_3, x_1x_3$, i.e. $M5, M6, M7$
- 3 covariates: $x_1x_2x_3$, i.e $M8$

So, 

$$
P(M = M2) = P(M \in \{M2,M3,M4\})\times P(M2|M \in \{M2,M3,M4\})
$$

:::

## Integrated likelihood methods

- Return @eq-03 and @eq-04. We need to calculate: 

![](eq02.png){width=43% fig-align="center"}

- $\boldsymbol\beta_c$ can be integrated out analytically

![](eq03.png){width=43% fig-align="center"}

where $\boldsymbol H_c = \sum^{Q_c}_j (\tau_{cj}\boldsymbol Z_{cj}\boldsymbol\Sigma_{cj}\boldsymbol Z_{cj}^{\top}) + \widehat{\boldsymbol V}^{-1}$. $\boldsymbol\tau_c$ can't be integrated out analytically. 

## Integrated likelihood methods

::: {.panel-tabset}

### A

We need to approximate the integration wrt $\boldsymbol\tau_c$ using Laplace method. 

- Transform $\boldsymbol\delta_c = \ln\boldsymbol\tau_c$.
- Integrate out $\boldsymbol\delta_c$:

$$
\int p(\boldsymbol y^*,\boldsymbol\tau_c|M_c)d\boldsymbol\tau_c = \int p(\boldsymbol y^*, e^{\boldsymbol\tau_c}|M_c)e^{\boldsymbol\tau_c}d\boldsymbol\tau_c
\approx (2\pi)^{Q_c/2} |q''(\widehat{\boldsymbol\delta}_c)|^{-1/2}e^{-q(\widehat{\boldsymbol\delta}_c)}
$$


![](eq04.png){width=70% fig-align="center"}

### B

#### Laplace approximation

We will be concerned with integrals of the form 

$$
I(t) = \int_K h(\boldsymbol x)e^{-tf(\boldsymbol x)}d\boldsymbol x
$$
Then, as $t$ tends to infinity, we have the following asymptotic equivalent 

$$
I(t) \approx \frac{h(\boldsymbol x_*)}{\sqrt{det[f''(\boldsymbol x_*)]}}\Big(\frac{2\pi}{t}\Big)^{d/2}e^{-tf(\boldsymbol x_*)}
$$

**Where does it come from?** [go to this Link](https://francisbach.com/laplace-method/)

:::

## Fractional Bayes factors

We now calculate the posterior model probabilities of interest. The the baseline model $M_l$ be the model with the largest integrated likelihood in the model space. The Bayes factor 

$$
BF_{cl} = \frac{p(\boldsymbol y^*|M_c)}{p(\boldsymbol y^*|M_l)}
$$

Hence,

$$
p(M_c|\boldsymbol y^*) \propto \frac{p(M_c)p(\boldsymbol y^*|M_c)}{p(\boldsymbol y^*|M_l)} \propto BF_{cl}p(M_c)
$$

since $\pi(\boldsymbol\beta_c|M_c)$ is improper, $p(\boldsymbol y^*|M_c)$ is only defined up to an unspecified constant of proportionality, and hence $BF_{cl}$. Models cannot be used to compared directly. 

- We then use the fractional bayes factor (FBF) to approximate the Bayes factor. 
- FBF is used to train the improper prior to obtain a meaningful Bayes factor, i.e we combine the improper prior with a fraction of the likelihood to obtain a proper distribution. 
- We then use the trained prior to compute the meaningful Bayes factor. 


## Fractional Bayes factors

We train the prior with a fraction $b$ of the likelihood. The trained prior is 

$$
\pi^b (\boldsymbol\beta_c,\boldsymbol\tau_c) = \frac{p^b(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi(\boldsymbol\beta_c,\boldsymbol\tau_c|M_c)}{\int p^b(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi(\boldsymbol\beta_c,\boldsymbol\tau_c|M_c)d\boldsymbol\beta_c}
$$

Following O'hagan (1995), the fractional integrated likelihood is equal

$$
\begin{aligned}
q_c(b,\boldsymbol y^*) &= \int p^{1-b}(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi^b(\boldsymbol\beta_c,\boldsymbol\tau_c)d\boldsymbol\beta_cd\boldsymbol\tau_c\\
&= \int p^{1-b}(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)
\frac{p^b(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi(\boldsymbol\beta_c,\boldsymbol\tau_c|M_c)}{\int p^b(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi(\boldsymbol\beta_c,\boldsymbol\tau_c|M_c)d\boldsymbol\beta_c}d\boldsymbol\beta_cd\boldsymbol\tau_c\\
&= \frac{\int p(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi(\boldsymbol\beta_c,\boldsymbol\tau_c|M_c)d\boldsymbol\beta_c\boldsymbol\tau_c}{\int p^b(\boldsymbol y^*|\boldsymbol\beta_c,\boldsymbol\tau_c)\pi(\boldsymbol\beta_c,\boldsymbol\tau_c|M_c)d\boldsymbol\beta_c\boldsymbol\tau_c}
\end{aligned}
$$

-  In GLMM applications, $b = (p+1)/n$ yields well-defined Bayes factor. 

## Fractional Bayes factors

The FBF of $M_c$ versus $M_l$ is defined as 

$$
BF_{cl}^b = \frac{q_c(b,\boldsymbol y^*)}{q_l(b, \boldsymbol y^*). }
$$

The posterior probability of $M_c$ is

$$
p^b(M_c|\boldsymbol y) = BF^b_{cl} \times \frac{P(M_c)}{\sum^C_{k=1}BF_{kl}^b\times P(M_k)}
$$

# Simulation study

## 

<iframe width="100%" height="100%" src="BIOS612_PaperDiscussion_2.pdf"></iframe>

## References






